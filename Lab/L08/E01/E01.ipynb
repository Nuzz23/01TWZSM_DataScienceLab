{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - MODULES AND CONSTANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODULES, IMPORTS AND LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mathematical imports\n",
    "from math import sin\n",
    "\n",
    "# Testing and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, max_error, mean_absolute_percentage_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "\n",
    "# Regressors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSTANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the functions to be generated\n",
    "<br>\n",
    "- $f1(x) = x Â· sin(x) + 2x $\n",
    "<br>\n",
    "- $f2(x) = 10 sin(x) + x^2$\n",
    "<br>\n",
    "- $f3(x) = sign(x)(x^2 + 300) + 20 sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical function and number generating function\n",
    "FUNCTIONS = {   'f1':(lambda x: x*sin(x)+2*x), \n",
    "                'f2':(lambda x: 10*sin(x)+x**2), \n",
    "                'f3':(lambda x: (1 if x >= 0 else -1)*(x**2+300) + 20*sin(x))}\n",
    "\n",
    "OTHER_FUNCTIONS = { 'sin(x)':(lambda x: sin(x)), \n",
    "                    'x*sin(x)' : (lambda x: x*sin(x)), \n",
    "                    'x**2':(lambda x : x**2), \n",
    "                    'sign(x)*x**2':(lambda x: x**2 if x>=0 else -x**2)}\n",
    "\n",
    "REGRESSORS = [  LinearRegression(),RandomForestRegressor(), MLPRegressor(), \n",
    "                DecisionTreeRegressor(), KNeighborsRegressor(), Ridge(), Lasso(), ElasticNet()]\n",
    "\n",
    "\n",
    "METRICS = [explained_variance_score, max_error, mean_absolute_percentage_error, mean_squared_error, r2_score, root_mean_squared_error]\n",
    "\n",
    "ADD_NOISE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generates the data for each function and returns a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData(Xmin:int, Xmax:int, totN:int, functionsDict:dict=FUNCTIONS)->pd.DataFrame:\n",
    "    x = np.linspace(Xmin, Xmax, totN) \n",
    "    if ADD_NOISE:\n",
    "        x = x + np.random.normal(0, 50, size=x.size)\n",
    "\n",
    "    \n",
    "    return pd.concat([  pd.Series(x, name='x'),\n",
    "                        pd.DataFrame([[functionsDict[function](point) for function in functionsDict] for point in x], columns=functionsDict.keys())], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adds the behavior of some non linear functions to x df for a better approximation of f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addOtherNonLinearFunction(df:pd.DataFrame, xMin:int, xMax:int, totN:int, functions:dict=OTHER_FUNCTIONS)->pd.DataFrame:\n",
    "    return pd.concat([ df, pd.DataFrame([[functions[function](point) for function in functions] for point in np.linspace(xMin, xMax, totN)], columns=functions.keys())], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can either plot a single function or all functions at once <br>\n",
    "Draw and inspect the shape of the function. Which regression model of those you know could achieve\n",
    "better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFunction(x:pd.Series, y:pd.DataFrame|pd.Series, names:list[str]=None, title:str=None)->None:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "    ax.set_title(title or 'functions')\n",
    "    ax.plot(x, y)\n",
    "    ax.legend(labels=names or y.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame now a regression task to your generated data. Start from the ordinary least squares Linear\n",
    "Regression. <br>\n",
    "\n",
    "Fit each model to the training data and predict the function value for each test point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testLinearRegressor(df:pd.Series, xMin:int, xMax:int, totN:int, functionTested:str, \n",
    "                func, nonLinearFunc:dict=OTHER_FUNCTIONS, \n",
    "                regressor:LinearRegression=LinearRegression(n_jobs=-1, fit_intercept=True))->None:\n",
    "    xTest = np.linspace(xMin, xMax, totN)\n",
    "    \n",
    "    yPred = (regressor.fit(df[['x']+list(nonLinearFunc.keys())], np.reshape(df[functionTested], (-1, 1)))\n",
    "            .predict(addOtherNonLinearFunction(pd.Series(xTest, name='x'), xMin, xMax, totN, OTHER_FUNCTIONS)))\n",
    "    \n",
    "    plotFunction(xTest, pd.concat([pd.Series(yPred[:, 0]), pd.Series(list(map(lambda x: func(x), xTest)))],axis=1), \n",
    "                 names=['yTrue', 'yPred'], title='Predicted vs True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, choose additional models which you believe could outperform linear regression to approximate the function.\n",
    "\n",
    "<br> Info: There are many regression models in scikit-learn, other than the ones that you should already be familiar with (e.g. LinearRegression, Ridge, SVR). Part of the models that you adopted\n",
    "for classification have their regression counterparts, such as MLPRegressor and RandomForestRegressor.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeOtherRegressor(df:pd.DataFrame, xMin:int, xMax:int, totN:int, functionTested:str, \n",
    "                nonLinearFunc:dict=OTHER_FUNCTIONS, regressor:DecisionTreeRegressor=DecisionTreeRegressor()) -> None:\n",
    "    \n",
    "    xTest = np.linspace(xMin, xMax, totN)\n",
    "\n",
    "    return  regressor.fit(\n",
    "        df[['x'] + list(nonLinearFunc.keys())],\n",
    "        np.reshape(df[functionTested], (-1, 1)).ravel(),\n",
    "    ).predict(\n",
    "        addOtherNonLinearFunction(\n",
    "            pd.Series(xTest, name='x'), xMin, xMax, totN, OTHER_FUNCTIONS\n",
    "        )\n",
    "    )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRIC COMPUTATION   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes a series of known metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(yTrue:pd.Series, yPred:pd.Series, metrics:list, index:str)->pd.DataFrame:\n",
    "    return pd.DataFrame([metric(yTrue, yPred) for metric in metrics], columns=[index[:-2]], \n",
    "                        index=list(map(lambda x: str(x).split()[1], metrics))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests the regressor given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOtherRegressor(df:pd.DataFrame, xMin:int, xMax:int, totN:int, functionTested:str='f1', func=FUNCTIONS['f1'],\n",
    "                nonLinearFunc:dict=OTHER_FUNCTIONS, regressors:list[DecisionTreeRegressor]=[DecisionTreeRegressor()]) -> pd.DataFrame:\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for regr in regressors:\n",
    "        if len(data):\n",
    "            data = pd.concat([computeMetrics(pd.Series(list(map(lambda y:func(y), np.linspace(xMin, xMax, totN)))),\n",
    "                    computeOtherRegressor(df, xMin, xMax, totN, functionTested, nonLinearFunc, regr), METRICS, str(regr)), data], axis=0)\n",
    "        else:\n",
    "            data = computeMetrics(pd.Series(list(map(lambda y:func(y), np.linspace(xMin, xMax, totN)))),\n",
    "                    computeOtherRegressor(df, xMin, xMax, totN, functionTested, nonLinearFunc, regr), METRICS, str(regr))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function of the program\n",
    "it does:\n",
    "<ol>\n",
    "<li>The generation of the data</li>\n",
    "<li>Plots the functions</li>\n",
    "<li>Trains and builds the regression model, after adding other non linear functions for a better approximation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utente\\OneDrive\\Desktop\\Magistrale\\01TWZSM_DataScienceLab\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explained_variance_score</th>\n",
       "      <th>max_error</th>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>r2_score</th>\n",
       "      <th>root_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.881689</td>\n",
       "      <td>24.052292</td>\n",
       "      <td>0.177755</td>\n",
       "      <td>105.755015</td>\n",
       "      <td>0.841510</td>\n",
       "      <td>10.283726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.880991</td>\n",
       "      <td>24.110673</td>\n",
       "      <td>0.178146</td>\n",
       "      <td>106.155190</td>\n",
       "      <td>0.840911</td>\n",
       "      <td>10.303164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.856223</td>\n",
       "      <td>25.693865</td>\n",
       "      <td>0.193776</td>\n",
       "      <td>121.620573</td>\n",
       "      <td>0.817733</td>\n",
       "      <td>11.028172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.761395</td>\n",
       "      <td>0.542240</td>\n",
       "      <td>776.169602</td>\n",
       "      <td>-0.163206</td>\n",
       "      <td>27.859821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>-2.209149</td>\n",
       "      <td>151.970100</td>\n",
       "      <td>0.416924</td>\n",
       "      <td>2284.028990</td>\n",
       "      <td>-2.422958</td>\n",
       "      <td>47.791516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>0.934840</td>\n",
       "      <td>23.021806</td>\n",
       "      <td>0.123691</td>\n",
       "      <td>123.952913</td>\n",
       "      <td>0.814238</td>\n",
       "      <td>11.133414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>0.543335</td>\n",
       "      <td>47.082442</td>\n",
       "      <td>0.356969</td>\n",
       "      <td>425.673195</td>\n",
       "      <td>0.362065</td>\n",
       "      <td>20.631849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.855740</td>\n",
       "      <td>25.721647</td>\n",
       "      <td>0.194064</td>\n",
       "      <td>121.918224</td>\n",
       "      <td>0.817287</td>\n",
       "      <td>11.041659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       explained_variance_score   max_error  \\\n",
       "ElasticNet                             0.881689   24.052292   \n",
       "Lasso                                  0.880991   24.110673   \n",
       "Ridge                                  0.856223   25.693865   \n",
       "KNeighborsRegressor                    0.000000   48.761395   \n",
       "DecisionTreeRegressor                 -2.209149  151.970100   \n",
       "MLPRegressor                           0.934840   23.021806   \n",
       "RandomForestRegressor                  0.543335   47.082442   \n",
       "LinearRegression                       0.855740   25.721647   \n",
       "\n",
       "                       mean_absolute_percentage_error  mean_squared_error  \\\n",
       "ElasticNet                                   0.177755          105.755015   \n",
       "Lasso                                        0.178146          106.155190   \n",
       "Ridge                                        0.193776          121.620573   \n",
       "KNeighborsRegressor                          0.542240          776.169602   \n",
       "DecisionTreeRegressor                        0.416924         2284.028990   \n",
       "MLPRegressor                                 0.123691          123.952913   \n",
       "RandomForestRegressor                        0.356969          425.673195   \n",
       "LinearRegression                             0.194064          121.918224   \n",
       "\n",
       "                       r2_score  root_mean_squared_error  \n",
       "ElasticNet             0.841510                10.283726  \n",
       "Lasso                  0.840911                10.303164  \n",
       "Ridge                  0.817733                11.028172  \n",
       "KNeighborsRegressor   -0.163206                27.859821  \n",
       "DecisionTreeRegressor -2.422958                47.791516  \n",
       "MLPRegressor           0.814238                11.133414  \n",
       "RandomForestRegressor  0.362065                20.631849  \n",
       "LinearRegression       0.817287                11.041659  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main()->None:\n",
    "    df = generateData(-20, 20, 100, FUNCTIONS) # 1  \n",
    "    \n",
    "    # plotFunction(df['x'], df[df.columns.difference(['x'])]) # 2    \n",
    "    \n",
    "    df = addOtherNonLinearFunction(df, -20, 20, 100, OTHER_FUNCTIONS)\n",
    "    \n",
    "    # testLinearRegressor(df, 20, 40, 100, 'f1', FUNCTIONS['f1'], OTHER_FUNCTIONS) # 3\n",
    "    \n",
    "    display(testOtherRegressor(df, 20, 40, 100, 'f1', FUNCTIONS['f1'], OTHER_FUNCTIONS, REGRESSORS))\n",
    "        \n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
