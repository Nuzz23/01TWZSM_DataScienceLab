{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - MODULES AND CONSTANTS\n",
    "All the modules, constants, import and libraries used in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from seaborn import heatmap\n",
    "\n",
    "## Imports from sklearn\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "DATA_INPUT_FILE = 'wine.data'\n",
    "CLASSES = ['class']\n",
    "FEATURE_NAMES = ['Alcohol', 'Malicacid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols',\n",
    "'Proanthocyanins', 'Color_intensity', 'Hue', '0D280_0D315_of_diluted_wines', 'Proline']\n",
    "TEST_DATA_PERCENTAGE=0.2\n",
    "METRICS = ['accuracy_score', 'balanced_accuracy_score', 'balanced_accuracy_score_adjusted', 'f1_score', \n",
    "            'recall_score', 'precision_score']\n",
    "PRINT_METRIC_ANALYSIS_TO_EXCEL = 0 \n",
    "EXCEL_FILE = 'Configuration.xlsx'\n",
    "METRICS_SHEET = 'Metrics'\n",
    "FINAL_METRICS_SHEET = 'FinalMetrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - DATASET LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from sklearn, as described in Subsec. Then, based on your X and y, answer\n",
    "the following questions:\n",
    "- How many records are available?\n",
    "- Are there missing values?\n",
    "- How many elements does each class contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataBase(file:str, cols:list[str])->pd.DataFrame:\n",
    "    return pd.read_csv(filepath_or_buffer=file, delimiter=',', header=None, names=cols)\n",
    "\n",
    "def AnsQuestionPart1(df:pd.DataFrame)->None:\n",
    "    print('How many records are available ? \\t', df.shape[0])\n",
    "    print('Are there missing values ? \\n', df[df.isna()].count())\n",
    "    print('How many elements does each class contain? \\t', df.loc[:, 'class'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - CLASSIFIER BUILDING AND TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DecisionTreeClassifier object with the default configuration (i.e. without passing any\n",
    "parameters to the constructor). <br>Train the classifier using your X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndTrainTree(df:pd.DataFrame, features:list[str], classes:list[str])->DecisionTreeClassifier:\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X=df.loc[:, features], y=df.loc[:, classes])\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - TREE PLOTTING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created a tree, you can visualize it. Sklearn offers two functions to visualize decision trees. \n",
    "<ul>\n",
    "<li>The first one, plot_tree(), plots the tree in a matplotlib-based, interactive window.</li>\n",
    "<li>An alternative way is using export_graphviz(). This function exports the tree as a DOT file. DOT\n",
    "is a language for describing graph (and, as a consequence, trees). From the DOT code, you can\n",
    "generate the resulting visual representation either using specific Python libraries, or by using any\n",
    "online tools (such as Webgraphviz). </li>\n",
    "</ul>\n",
    "We recommend using the latter approach, where you paste the string returned by export_graphviz (which is the DOT file) directly into Webgraphviz.<br> If, instead, you would rather run it locally, you can install pydot (Python package) and graphviz (a graph\n",
    "visualization software). <br>\n",
    "After you successfully plotted a tree, you can take a closer look at the result and draw some conclusions. \n",
    "<br> In particular, what information is contained in each node? Take a closer look at the leaf\n",
    "nodes. <br>Based on what you know about over fitting, what can you learn from these nodes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this import are only used here, so they will be exceptionally defined here\n",
    "from pydot import graph_from_dot_data\n",
    "from IPython.display import Image\n",
    "\n",
    "def printTree(tree:DecisionTreeClassifier, features:list[str], printGood:bool)->None:\n",
    "    if printGood:\n",
    "        dot_code = export_graphviz(tree, feature_names=features)\n",
    "        Image(graph_from_dot_data(dot_code)[0].create_png())\n",
    "    else:\n",
    "        plot_tree(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - PREDICTION FOR OVER FITTING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset X, you can get the predictions of the classifier (one for each entry in X) by calling\n",
    "the predict() of DecisionTreeClassifier. <br>\n",
    "Then, use the accuracy_score() function (which you can import from sklearn.metrics) to compute the accuracy between two lists of values (y_true,\n",
    "the list of “correct” labels, and y_pred, the list of predictions made by the classifier). \n",
    "<br> Since you already have both these lists (y for the ground truth, and the result of the predict() method for the\n",
    "prediction), you can already compute the accuracy of your classifier. \n",
    "<br> What result do you get? \n",
    "<br> Does this result seem particularly high/low? \n",
    "<br> Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOverFittingValues(tree:DecisionTreeClassifier, df:pd.DataFrame, classes:list[str], features:list[str], method: object) -> float:\n",
    "    return method(df.loc[:, classes], tree.predict(df.loc[:, features])) *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - PARTITIONED DATA SET TESTING AND ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we can split our dataset into a training set and a test set. <br>\n",
    "We will use the training set to train a model, and to assess its performance with the test set. <br>\n",
    "Sklearn offers the train_test_split() function to split any number of arrays (all having the same length on the first dimension) into two\n",
    "sets. <br> You can use an 80/20 train/test split. If used correctly, you will get 4 arrays: X_train, X_test, y_train, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionAndTest(tree:DecisionTreeClassifier, df:pd.DataFrame, classes:list[str], features:list[str], testPercentage:float)->float:\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(df.loc[:, features], df.loc[:, classes], test_size=testPercentage)\n",
    "    tree.fit(xTrain, yTrain)\n",
    "    yPred = tree.predict(xTest)\n",
    "    \n",
    "    heatmap(confusion_matrix(yTest, yPred), annot=True)\n",
    "    \n",
    "    return round(accuracy_score(yTest, yPred),3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 -  ADVANCED METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a new model using (X_train, y_train). Then, compute the accuracy with (X_test,y_test). \n",
    "<br> How does this value compare to the previously computed one? Is this a more reasonable value? Why? <br>\n",
    "\n",
    "This should give you a good idea as to why training and testing on the same dataset returns meaningless results. \n",
    "\n",
    "You can also compute other metrics (e.g. precision, recall, F1 score) using the respective functions (precision_score, recall_score, f1_score). <br> Note that, since these three metrics are all based on a single class, you can either compute the value for a single class, aggregate\n",
    "the results into a single value, or receive the results for all three classes. \n",
    "<br>You can also use the classification_report function, which returns various metrics (including the previously mentioned ones) for each of the\n",
    "classes of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionAndTestWithAdvancedMetrics(tree:DecisionTreeClassifier, df:pd.DataFrame, classes:list[str], \n",
    "                                        features:list[str], testPercentage:float, printMatrix:bool=False)->dict[str:float]:\n",
    "        \n",
    "        xTrain, XTest, yTrain, yTest = train_test_split(df.loc[:, features], df.loc[:, classes], test_size=testPercentage)\n",
    "        tree.fit(xTrain, yTrain)\n",
    "        yPred = tree.predict(XTest)\n",
    "        \n",
    "        if printMatrix:\n",
    "                heatmap(confusion_matrix(yTest, yPred), annot=True)\n",
    "        \n",
    "        return {'accuracy_score':accuracy_score(yTest, yPred), 'balanced_accuracy_score':balanced_accuracy_score(yTest, yPred),\n",
    "                'balanced_accuracy_score_adjusted':balanced_accuracy_score(yTest, yPred, adjusted=True), \n",
    "                'f1_score':f1_score(yTest, yPred, labels=[1,2,3], average=None), 'recall_score':recall_score(yTest, yPred, average=None), \n",
    "                'precision_score':precision_score(yTest, yPred, average=None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - ADVANCED TREE SETTINGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you have only used “default” decision trees, but, the “default” decision tree might not be the best option in terms of performance to fit our dataset. <br>\n",
    "In this exercise, you will perform a “grid search”: you will define a set of possible configurations and, for each configuration, build a classifier. Then, you will test the performance of each classifier and identify that configuration that produces the best model.<br>\n",
    "On the official documentation for DecisionTreeClassifier you can find a list of all parameters you\n",
    "can modify. Identify some of the parameters that, based on your theoretical knowledge of decision\n",
    "trees, might affect the performance of the tree. <br>\n",
    "For each of these parameters, define a set of possible\n",
    "values (the official documentation provides additional information about the possible values that can\n",
    "be used). For example, we can identify these two parameters:\n",
    "<ul>\n",
    "<li><b>max_depth</b>: which defines the maximum depth of the decision tree, can be set to None (i.e.\n",
    "unbounded depth), or to values such as 2, 4, 8 (we already know from previous exercises the\n",
    "approximate depth the tree can reach with this dataset) </li>\n",
    "<li><b>splitter</b>: which can be set to either best (in which case, for each split, the algorithm will try\n",
    "all possible splits), or random (in this case, the algorithm will try N random splits on various\n",
    "features and select the best one)\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "You can and should identify additional parameters and possible values for them. Then, you can\n",
    "build a parameter dictionary (i.e. a dictionary where keys are parameter names and values are lists\n",
    "of candidate values). Using the ParameterGrid class offered by scikit-learn, you can generate a list\n",
    "of all possible configurations that can be obtained from the parameter dictionary.<br>\n",
    "For each configuration config, we can train a separate model with our training data, and validate\n",
    "it with our test data: for each configuration, compute the resulting accuracy on the test data. Then,\n",
    "select the parameter configuration having highest accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advancedTreeBuilding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    param = ParameterGrid({\n",
    "        \"max_depth\": [None, 2, 4, 8],\n",
    "        \"splitter\": [\"best\", \"random\"],\n",
    "        \"criterion\":['gini', 'entropy']\n",
    "    })\n",
    "    \n",
    "    ris = (pd.DataFrame(partitionAndTestWithAdvancedMetrics(DecisionTreeClassifier(**param[0]), df, CLASSES, FEATURE_NAMES, TEST_DATA_PERCENTAGE))\n",
    "                .set_index(keys=pd.MultiIndex.from_arrays([[' - '.join([f\"{param[0][k]}\" for k in (\"max_depth\", \"splitter\", \"criterion\")])]*3, [1,2,3]], \n",
    "                                                        names=['configuration [max_depth - splitter - criterion]', 'class'])))\n",
    "    \n",
    "    for i in range(1,len(param)):\n",
    "        ris = pd.concat([ris, pd.DataFrame(partitionAndTestWithAdvancedMetrics(DecisionTreeClassifier(**(param[i])), df, CLASSES, FEATURE_NAMES, TEST_DATA_PERCENTAGE))\n",
    "                .set_index(keys=pd.MultiIndex.from_arrays([[' - '.join([f\"{param[i][k]}\" for k in (\"max_depth\", \"splitter\", \"criterion\")])]*3, [1,2,3]], \n",
    "                                                        names=['configuration [max_depth - splitter - criterion]', 'class']))])\n",
    "    \n",
    "    return ris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, you searched for the best configuration among a list of possible alternatives. <br>\n",
    "Since we used our test data to select the model, you may be overfitting on the test data (you may\n",
    "have selected the configuration that works best for the test set, but which may not be as good on\n",
    "new data). <br>\n",
    "Typically, you do not want to use the test set for tuning the model’s hyperparameters,\n",
    "since the test set should only be used as a final evaluation.<br>\n",
    "For this reason, datasets are typically split into:\n",
    "<ul>\n",
    "<li>Training set: used to create the model.</li>\n",
    "<li>Validation set: used to assess how good each configuration of a classifier is.</li>\n",
    "<li>Test set: used at the end of the hyperparameter tuning, to assess how good our final model is.</li>\n",
    "</ul>\n",
    "<br>\n",
    "However, it often happens that only a limited amount of data is available. In these cases, it is wasteful\n",
    "to only use a small fraction of the dataset for the actual training. In these cases, cross-validation can\n",
    "be used to “get rid” of the validation set. One popular method of is the k-fold cross-validation. <br>\n",
    "In this, the training set is split into k partitions. \n",
    "k − 1 are used for the training, the other one is used validation. This is repeated until all\n",
    "partitions have been used once for validation.\n",
    "<br>\n",
    "For each fold, you can use the training data to train each classifier and measure the performance on the\n",
    "validation data (i.e. X_valid).<br> \n",
    "You can then aggregate the information extracted  and select the best performing model. After\n",
    "you select one model, you can assess its performance on never-before-seen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for building and testing the given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMetrics(tree:DecisionTreeClassifier, xTest:pd.DataFrame, yTest:pd.DataFrame, printHeatMap:bool=False)->dict[str:float]:\n",
    "    yPred = tree.predict(xTest)\n",
    "    \n",
    "    if printHeatMap:\n",
    "        heatmap(confusion_matrix(yTest, yPred), annot=True)\n",
    "        \n",
    "    return {'accuracy_score':accuracy_score(yTest, yPred), 'balanced_accuracy_score':balanced_accuracy_score(yTest, yPred),\n",
    "                'balanced_accuracy_score_adjusted':balanced_accuracy_score(yTest, yPred, adjusted=True), \n",
    "                'f1_score':f1_score(yTest, yPred, labels=[1,2,3], average=None), 'recall_score':recall_score(yTest, yPred, average=None), \n",
    "                'precision_score':precision_score(yTest, yPred, average=None)}\n",
    "\n",
    "\n",
    "def advancedTreeBuilding(xTrain: pd.DataFrame, xTest: pd.DataFrame, yTrain:pd.DataFrame, yTest: pd.DataFrame, \n",
    "                        iter:int) -> pd.DataFrame:\n",
    "    param = ParameterGrid({\n",
    "        \"max_depth\": [None, 2, 4, 8],\n",
    "        \"splitter\": [\"best\", \"random\"],\n",
    "        \"criterion\":['gini', 'entropy']\n",
    "    })\n",
    "    \n",
    "    ris = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(param)):\n",
    "        if len(ris) <= 1:\n",
    "            ris=(pd.DataFrame(testMetrics(DecisionTreeClassifier(**(param[i])).fit(xTrain, yTrain), xTest, yTest))\n",
    "            .set_index(keys=pd.MultiIndex.from_arrays([[' - '.join([f\"{param[i][k]}\" for k in (\"max_depth\", \"splitter\", \"criterion\")])]*3, [iter]*3, [1,2,3]], \n",
    "                                                            names=['configuration [max_depth - splitter - criterion]', 'iteration','class']))   \n",
    "            )\n",
    "        else:\n",
    "            ris = pd.concat([ris, \n",
    "                            (pd.DataFrame(testMetrics(DecisionTreeClassifier(**(param[i])).fit(xTrain, yTrain), xTest, yTest))\n",
    "            .set_index(keys=pd.MultiIndex.from_arrays([[' - '.join([f\"{param[i][k]}\" for k in (\"max_depth\", \"splitter\", \"criterion\")])]*3, [iter]*3, [1,2,3]], \n",
    "                                                            names=['configuration [max_depth - splitter - criterion]', 'iteration','class']))   \n",
    "            )], axis=0)\n",
    "    return ris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for doing the cross validation of our data and the printing on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(df:pd.DataFrame, features:list[str], classes:list[str], testPercentage:float, crossValidationSize:int=5\n",
    "                    )->list[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # Split the datasets into two:\n",
    "    #   X_train_valid: the dataset used for the k-fold cross-validation\n",
    "    #   X_test: the dataset used for the final testing (this will NOT be seen by the classifier during the training/validation phases)\n",
    "    \n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(df.loc[:, features], df.loc[:, classes], test_size=testPercentage)\n",
    "    \n",
    "    kf = KFold(n_splits=crossValidationSize)\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    for i, (train_indices, validation_indices) in enumerate(kf.split(X_train_valid)):\n",
    "        xTrain = X_train_valid.iloc[train_indices]\n",
    "        xValid = X_train_valid.iloc[validation_indices]\n",
    "        yTrain = y_train_valid.iloc[train_indices]\n",
    "        yValid = y_train_valid.iloc[validation_indices]\n",
    "        if len(data) <= 1: \n",
    "            data = advancedTreeBuilding(xTrain, xValid, yTrain, yValid, i)\n",
    "        else:\n",
    "            data = pd.concat([data, advancedTreeBuilding(xTrain, xValid, yTrain, yValid, i)])\n",
    "    \n",
    "    temp = ''\n",
    "    for index in data.index.get_level_values(0).unique():\n",
    "        if len(temp) <=1:\n",
    "            temp = data.loc[index].agg(func=['min', 'max', 'mean', 'std']).set_index(keys=\n",
    "                    pd.MultiIndex.from_arrays([[index]*4, ['min', 'max', 'mean', 'std']]))\n",
    "        else:\n",
    "            temp = pd.concat([temp, data.loc[index].agg(func=['min', 'max', 'mean', 'std']).set_index(keys=\n",
    "                    pd.MultiIndex.from_arrays([[index]*4, ['min', 'max', 'mean', 'std']]))], axis=0)\n",
    "        \n",
    "    \n",
    "    temp['avgFinal'] = np.zeros(temp.shape[0])\n",
    "    for index in temp.index:\n",
    "        temp.loc[index, 'avgFinal'] = temp.loc[index].agg('mean')\n",
    "    \n",
    "    return (data.sort_index(), temp.sort_index(), X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prints the best possible hyper parameters for the configuration of the decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBestConfiguration(df:pd.DataFrame)->DecisionTreeClassifier:\n",
    "    config = df.loc[list(filter(lambda x : x[1]=='mean' , df.index)), 'avgFinal'].idxmax()[0]\n",
    "    \n",
    "    print(\"Best configuration : \", config)\n",
    "    config = config.strip().split('-')\n",
    "    \n",
    "    \n",
    "    return DecisionTreeClassifier(max_depth=int(config[0].strip()) if config[0].strip() != 'None' else None, \n",
    "                    splitter=config[1].strip(), criterion=config[2].strip( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the main function of our program that will coordinate code execution, it does:\n",
    "<ol>\n",
    "<li>Loads the database into a data structure</li>\n",
    "<li>Creates and trains the classifier</li>\n",
    "<li>Prints the tree</li>\n",
    "<li>Accuracy evaluation for over fitting</li>\n",
    "<li>Model testing with partitioned data set</li>\n",
    "<li>Advanced metrics</li>\n",
    "<li>Advanced and custom tree classifier usage</li>\n",
    "<li>Validating the model trough cross validation</li>\n",
    "<li></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utente\\OneDrive\\Desktop\\Magistrale\\01TWZSM_DataScienceLab\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration :  8 - best - gini\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>balanced_accuracy_score_adjusted</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>precision_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.948413</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy_score  balanced_accuracy_score  balanced_accuracy_score_adjusted  \\\n",
       "0        0.944444                 0.948413                          0.922619   \n",
       "1        0.944444                 0.948413                          0.922619   \n",
       "2        0.944444                 0.948413                          0.922619   \n",
       "\n",
       "   f1_score  recall_score  precision_score  \n",
       "0  0.962963      0.928571         1.000000  \n",
       "1  0.916667      0.916667         0.916667  \n",
       "2  0.952381      1.000000         0.909091  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main()->None:\n",
    "    df = loadDataBase(DATA_INPUT_FILE, CLASSES+FEATURE_NAMES) # 1\n",
    "    # AnsQuestionPart1(df) # 1\n",
    "    \n",
    "    tree = createAndTrainTree(df, FEATURE_NAMES, classes=['class']) # 2\n",
    "    \n",
    "    # printTree(tree, FEATURE_NAMES, False) # 3\n",
    "    \n",
    "    # print(\"'DUMB' Accuracy of the model \", testOverFittingValues(tree, df, CLASSES, FEATURE_NAMES, accuracy_score)) # 4\n",
    "    \n",
    "    # print(\"final accuracy = \", partitionAndTest(DecisionTreeClassifier(), df, CLASSES, FEATURE_NAMES, TEST_DATA_PERCENTAGE)) # 5\n",
    "    \n",
    "    # display(pd.DataFrame(partitionAndTestWithAdvancedMetrics(DecisionTreeClassifier(), df, CLASSES, FEATURE_NAMES, TEST_DATA_PERCENTAGE, True))) # 6\n",
    "    \n",
    "    # display(advancedTreeBuilding(df)) # 7\n",
    "    \n",
    "    (metrics, finalMetrics, _, _) = crossValidation(df, FEATURE_NAMES, CLASSES, TEST_DATA_PERCENTAGE) # 7\n",
    "    \n",
    "    if PRINT_METRIC_ANALYSIS_TO_EXCEL:\n",
    "      writer = pd.ExcelWriter(path=EXCEL_FILE, mode='a', if_sheet_exists='replace') \n",
    "      metrics.to_excel(excel_writer=writer, sheet_name=METRICS_SHEET, float_format='%.4f')\n",
    "      finalMetrics.to_excel(excel_writer=writer, sheet_name=FINAL_METRICS_SHEET, float_format='%.4f')\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(df.loc[:, FEATURE_NAMES], df.loc[:, CLASSES], test_size=TEST_DATA_PERCENTAGE) # 8\n",
    "    display(pd.DataFrame(testMetrics(createBestConfiguration(finalMetrics).fit(xTrain, yTrain), xTest, yTest, True))) # 8\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
