{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are listed all the modules, libraries, imports and constant used for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EnumReviews import enumReviews\n",
    "import string\n",
    "\n",
    "\n",
    "FILE_INPUT = \"reviews.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to read the file as a list of list of words (strings) and label (integer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path2file:str)->list[list[str, int]]:\n",
    "    data = []\n",
    "    \n",
    "    with open(path2file, \"r\", encoding=\"UTF-8\") as fp:\n",
    "        fp.readline()\n",
    "        \n",
    "        for line in fp:\n",
    "            line=line.strip().split(\",\")\n",
    "            data.append([\",\".join(line[:-1]), int(line[-1])])\n",
    "          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function applies the tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data:list[list[str, int]])->list[list[str]]:\n",
    "    \"\"\"Compute the tokens for each document.\n",
    "    Input: a list of strings. Each item is a document to tokenize.\n",
    "    Output: a list of lists. Each item is a list containing the tokens of the\n",
    "    relative document.\n",
    "    \"\"\"\n",
    "    enum = enumReviews()\n",
    "    tokens = []\n",
    "    for doc in data:\n",
    "        doc = doc[enum.DESC]\n",
    "        for punct in string.punctuation:\n",
    "            doc = doc.replace(punct, \" \")\n",
    "        split_doc = [token.lower() for token in doc.split(\" \") if token]\n",
    "        tokens.append(split_doc)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function counts the number of occurrence of words in a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countFrequencies(token:list[list[str]])->list[dict[str:int]]:\n",
    "    # sourcery skip: inline-immediately-returned-variable, move-assign-in-block\n",
    "    # No good it's cubic !\n",
    "    # return [{word:token[i].count(word) for word in token[i]} for i in range(len(token))]\n",
    "\n",
    "    # This is quadratic, which is better\n",
    "    data = []\n",
    "    for text in token:\n",
    "        diz = dict()\n",
    "        for word in text:\n",
    "            if word in diz:\n",
    "                diz[word]+=1\n",
    "            else:\n",
    "                diz[word]=1\n",
    "        data.append(diz)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the df of each word in a list of list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDF(freq:list[dict[str:int]], words:set[str])->dict[str:int]:\n",
    "    pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the IDF for each word in the text and returns the list in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(freq:list[dict[str:int]])->list[list[str, float]]:\n",
    "    df = computeDF(freq, {key for doc in freq for key in doc.keys()})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main()->None:\n",
    "    data = load_file(FILE_INPUT)\n",
    "    token = tokenize(data)\n",
    "    frequencies = countFrequencies(token)\n",
    "    idf = computeIDF(frequencies)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
